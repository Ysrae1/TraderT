{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import pdb\n",
    "import time\n",
    "\n",
    "from ipywidgets import interact, IntSlider, Layout\n",
    "\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "_dtype_ = torch.float32\n",
    "_device_ = torch.device('cpu')\n",
    "\n",
    "# seq_len = 1\n",
    "# seq_len_forward = 1\n",
    "\n",
    "metrics = {}\n",
    "\n",
    "class PP_MLP(nn.Module): # Price Pridiction\n",
    "\n",
    "    def __init__(self, seq_len_f):\n",
    "\n",
    "        super(PP_MLP, self).__init__() \n",
    "        self.layer1 = nn.Linear(14, 20)  \n",
    "        self.layer2 = nn.Linear(20, 20)\n",
    "        self.layer3 = nn.Linear(20, 20)\n",
    "        self.output = nn.Linear(20, seq_len_f)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.relu(self.layer1(x)) \n",
    "        x = self.relu(self.layer2(x)) \n",
    "        x = self.relu(self.layer3(x)) \n",
    "        return self.output(x) \n",
    "    \n",
    "class PP_LSTM(nn.Module):\n",
    "    def __init__(self, seq_len_f):\n",
    "\n",
    "        super(PP_LSTM, self).__init__()\n",
    "        self.hidden_dim = 20\n",
    "        self.lstm = nn.LSTM(14, self.hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.dense_1 = nn.Linear(self.hidden_dim, 20)\n",
    "        self.dense_2 = nn.Linear(20, 20)\n",
    "        self.dense_3 = nn.Linear(20, seq_len_f)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        x = lstm_out[:, -1, :]\n",
    "        x = self.dropout(x) \n",
    "        x = self.relu(self.dense_1(x))\n",
    "        x = self.relu(self.dense_2(x))\n",
    "        x = self.dense_3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class PP_LSTMSeq2Seq(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(PP_LSTMSeq2Seq, self).__init__()\n",
    "\n",
    "        emb_size = 64\n",
    "\n",
    "        self.embedding_encoder = nn.Linear(14, emb_size)\n",
    "        self.embedding_decoder = nn.Linear(1, emb_size)\n",
    "\n",
    "        self.mlp_emb = nn.Sequential(\n",
    "            nn.Linear(emb_size, emb_size),\n",
    "            nn.LayerNorm(emb_size),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(emb_size, emb_size)\n",
    "        )\n",
    "\n",
    "        self.hidden_dim = 20\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=emb_size,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.25\n",
    "        )\n",
    "\n",
    "        self.dense_1 = nn.Linear(self.hidden_dim, 10)\n",
    "        self.dense_2 = nn.Linear(10, 5)\n",
    "        self.dense_3 = nn.Linear(5, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if x.shape[-1] == 1:\n",
    "            x = self.embedding_decoder(x)\n",
    "        else:\n",
    "            x = self.embedding_encoder(x)\n",
    "        \n",
    "        x = self.mlp_emb(x)\n",
    "\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        x = lstm_out\n",
    "        x = self.relu(self.dense_1(x))\n",
    "        x = self.relu(self.dense_2(x))\n",
    "        x = self.dense_3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "        \n",
    "class H5Dataset(Dataset):\n",
    "    def __init__(self, file_path, norm_flag, max_samples=None):\n",
    "        self.file_path = file_path\n",
    "        self.dataset = {}\n",
    "\n",
    "        with h5py.File(self.file_path, 'r') as file:\n",
    "            len_data_original = len(file['inputs'])\n",
    "\n",
    "        self.max_samples = max_samples if max_samples is not None else len_data_original\n",
    "\n",
    "        with h5py.File(self.file_path, 'r') as file:\n",
    "            self.dataset['inputs'] = file['inputs'][:self.max_samples]\n",
    "            self.dataset['outputs'] = file['outputs'][:self.max_samples]\n",
    "\n",
    "        if norm_flag == 'n':\n",
    "            self.normalization()\n",
    "        if norm_flag == 'd':\n",
    "            pass\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset['inputs'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_data = torch.tensor(self.dataset['inputs'][idx], dtype=torch.float32)\n",
    "        output_data = torch.tensor(self.dataset['outputs'][idx], dtype=torch.float32)\n",
    "        return input_data, output_data\n",
    "    \n",
    "    def normalization(self):\n",
    "        inputs = self.dataset['inputs']\n",
    "        outputs = self.dataset['outputs']\n",
    "        min_vals_i = inputs.min(axis=0)  \n",
    "        max_vals_i = inputs.max(axis=0)\n",
    "        if outputs.ndim != 1:\n",
    "            min_vals_o = np.min(outputs)  \n",
    "            max_vals_o = np.max(outputs)   \n",
    "        else:\n",
    "            min_vals_o = outputs.min(axis=0)  \n",
    "            max_vals_o = outputs.max(axis=0)         \n",
    "\n",
    "        ranges_i = max_vals_i - min_vals_i\n",
    "        ranges_i[ranges_i == 0] = 1\n",
    "\n",
    "        if (max_vals_o - min_vals_o)!= 0:\n",
    "            ranges_o = max_vals_o - min_vals_o\n",
    "        else:\n",
    "            ranges_o = 1\n",
    "\n",
    "        self.dataset['inputs'] = (inputs - min_vals_i) / ranges_i\n",
    "        self.dataset['outputs'] = ((outputs - min_vals_o) / ranges_o).squeeze()\n",
    "\n",
    "    def add_indices(self):\n",
    "        inputs = self.dataset['inputs']\n",
    "        indices = np.arange(inputs.shape[0]).reshape(-1, 1)\n",
    "        self.dataset['inputs'] = np.hstack((indices, inputs))\n",
    "\n",
    "def set_learning_rate(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def denormalize_i(dataset, model, input):\n",
    "    outputs = dataset.dataset['outputs']\n",
    "    min_vals_o = outputs.min(axis=0)  \n",
    "    max_vals_o = outputs.max(axis=0)         \n",
    "\n",
    "    range_o = max_vals_o - min_vals_o\n",
    "\n",
    "    return (model(input)*range_o)+min_vals_o\n",
    "\n",
    "def denormalize_o(dataset, output):\n",
    "\n",
    "    outputs = dataset.dataset['outputs']\n",
    "    min_vals_o = outputs.min(axis=0)  \n",
    "    max_vals_o = outputs.max(axis=0)         \n",
    "\n",
    "    range_o = max_vals_o - min_vals_o\n",
    "\n",
    "    return (output*(range_o[:output.shape[1]]))+ min_vals_o[:output.shape[1]]\n",
    "\n",
    "\n",
    "def seq_gen(dataset, inputs, seq_len, device):\n",
    "\n",
    "    inputs_o = torch.zeros(inputs.shape[0], seq_len, inputs.shape[1]-1)\n",
    "\n",
    "    for i in range(inputs.shape[0]):\n",
    "\n",
    "        idx_i = int(inputs[i][0].item())\n",
    "        start_idx = max(0, idx_i - seq_len + 1)\n",
    "        end_idx = idx_i + 1\n",
    "        segment = dataset.dataset['inputs'][start_idx:end_idx, 1:]\n",
    "\n",
    "        first_col = segment[:, 0]\n",
    "        diff = np.diff(first_col)\n",
    "        last_increasing_idx = np.where(diff <= 0)[0]\n",
    "        if len(last_increasing_idx) > 0:\n",
    "            cut_point = last_increasing_idx[-1] + 1\n",
    "        else:\n",
    "            cut_point = 0\n",
    "\n",
    "        segment[:cut_point, :] = 0\n",
    "\n",
    "        segment_tensor = torch.from_numpy(segment)\n",
    "\n",
    "        fill_start = seq_len - (end_idx - start_idx)\n",
    "\n",
    "        target_size = seq_len - fill_start\n",
    "        \n",
    "        if segment_tensor.shape[0] < target_size:\n",
    "            padding_size = target_size - segment_tensor.shape[0]\n",
    "            padding = torch.zeros(padding_size, segment_tensor.shape[1])\n",
    "            segment_tensor = torch.cat([padding, segment_tensor], dim=0)\n",
    "        \n",
    "        inputs_o[i, fill_start:seq_len, :] = segment_tensor\n",
    "\n",
    "    return inputs_o.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    seq_len = int(sys.argv[1])\n",
    "    seq_len_forward = int(sys.argv[2])\n",
    "\n",
    "    dataset_train = H5Dataset(f'../data/lob_data/lob_data_train_f{seq_len_forward}.h5','n')\n",
    "    dataset_train_dn = H5Dataset(f'../data/lob_data/lob_data_train_f{seq_len_forward}.h5','d')\n",
    "\n",
    "    dataset_verif = H5Dataset(f'../data/lob_data/lob_data_verif_f{seq_len_forward}.h5','n')\n",
    "    dataset_verif_dn = H5Dataset(f'../data/lob_data/lob_data_verif_f{seq_len_forward}.h5','d')\n",
    "\n",
    "    dataset_test = H5Dataset(f'../data/lob_data/lob_data_test_f{seq_len_forward}.h5','n')\n",
    "    dataset_test_dn = H5Dataset(f'../data/lob_data/lob_data_test_f{seq_len_forward}.h5','d')\n",
    "\n",
    "    dataset_train.add_indices()\n",
    "    dataset_verif.add_indices()\n",
    "    dataset_test.add_indices()\n",
    "\n",
    "    dataset_train_dn.add_indices()\n",
    "    dataset_verif_dn.add_indices()\n",
    "    dataset_test_dn.add_indices()\n",
    "\n",
    "\n",
    "    PPLSTM_model = PP_LSTMSeq2Seq().to(_device_)\n",
    "\n",
    "    batch_size = 512\n",
    "\n",
    "    loader_train = DataLoader(dataset_train, batch_size = batch_size, shuffle = True)\n",
    "    loader_verif = DataLoader(dataset_verif, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(PPLSTM_model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "\n",
    "    model_save_path_LSTM = f'PPLSTMS2S_model_r{seq_len}_f{seq_len_forward}.pt'\n",
    "    optimizer_save_path_LSTM = f'PPLSTMS2S_optimizer_r{seq_len}_f{seq_len_forward}.pt'\n",
    "\n",
    "    # LSTM Training using Time Window\n",
    "\n",
    "    print(\"Start training from scratch.\")\n",
    "\n",
    "    epoch_losses_train = []\n",
    "    epoch_losses_verif = []\n",
    "\n",
    "    max_epochs = 50\n",
    "\n",
    "    patience = 5\n",
    "\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "    filename = f'LSTMS2S_r{seq_len}_f{seq_len_forward}_loss_train & verif_{timestamp}.dat'\n",
    "\n",
    "    filename_t = f'LSTMS2S_r{seq_len}_f{seq_len_forward}_time_cost_{timestamp}.dat'\n",
    "\n",
    "\n",
    "    best_loss_verif = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "\n",
    "        PPLSTM_model.train()\n",
    "\n",
    "        t_start = time.time()\n",
    "\n",
    "        running_loss_train = 0.0\n",
    "\n",
    "        print(f\"Start epoch {epoch+1}.\\n\")\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(loader_train):\n",
    "            \n",
    "            if seq_len_forward == 1:\n",
    "                inputs, labels = seq_gen(dataset_train, inputs, seq_len, _device_), labels.to(_device_).unsqueeze(1).unsqueeze(2)\n",
    "            else:\n",
    "                inputs, labels = seq_gen(dataset_train, inputs, seq_len, _device_), labels.to(_device_).unsqueeze(2)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs_encoder = PPLSTM_model(inputs)[:, -1, :]\n",
    "\n",
    "            outputs = torch.zeros([inputs.shape[0],seq_len_forward,1]).to(_device_)\n",
    "\n",
    "            outputs[:,0,:] = outputs_encoder\n",
    "\n",
    "            outputs[:,1:,:] = PPLSTM_model(labels)[:,:-1,:]\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(PPLSTM_model.parameters(), max_norm = 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss_train += loss.item()\n",
    "\n",
    "            if ((batch_idx+1) % (500*(4096//batch_size)) == 0) or batch_idx == 0:\n",
    "\n",
    "                print(f\"Epoch {epoch+1}, Batch {batch_idx+1}, Batch Loss_train: {loss.item()}\")\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "        PPLSTM_model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            running_loss_verif = 0.0\n",
    "\n",
    "            for batch_idx, (inputs, labels) in enumerate(loader_verif):\n",
    "\n",
    "                if seq_len_forward == 1:\n",
    "                    inputs, labels = seq_gen(dataset_verif, inputs, seq_len, _device_), labels.to(_device_).unsqueeze(1).unsqueeze(2)\n",
    "                else:\n",
    "                    inputs, labels = seq_gen(dataset_verif, inputs, seq_len, _device_), labels.to(_device_).unsqueeze(2)\n",
    "                    \n",
    "                outputs_encoder = PPLSTM_model(inputs)[:, -1, :]\n",
    "\n",
    "                outputs = torch.zeros([inputs.shape[0],seq_len_forward,1]).to(_device_)\n",
    "\n",
    "                outputs[:,0,:] = outputs_encoder\n",
    "\n",
    "                outputs[:,1:,:] = PPLSTM_model(labels)[:,:-1,:]\n",
    "\n",
    "                print(labels)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                running_loss_verif += loss.item()\n",
    "\n",
    "        epoch_loss_train = running_loss_train/len(loader_train)\n",
    "        epoch_loss_verif = running_loss_verif/len(loader_verif)\n",
    "\n",
    "        epoch_losses_train.append(epoch_loss_train)\n",
    "        epoch_losses_verif.append(epoch_loss_verif)\n",
    "\n",
    "        # scheduler_step.step()\n",
    "        with open(filename, 'a') as f:\n",
    "\n",
    "            f.write(f'Epoch {epoch+1}, Loss_train: {epoch_loss_train}, Loss_verif: {epoch_loss_verif}\\n')\n",
    "        print(f'Epoch {epoch+1}, Epoch Average Loss_train: {epoch_loss_train}, Epoch Average Loss_verif: {epoch_loss_verif}\\n')\n",
    "\n",
    "        t_end = time.time()\n",
    "        t_duration = t_end - t_start\n",
    "\n",
    "        with open(filename_t,'a') as f_t:\n",
    "            f_t.write(f'Epoch {epoch+1} cost {t_duration} s.\\n')\n",
    "\n",
    "        if epoch_loss_verif < best_loss_verif:\n",
    "            best_loss_verif = epoch_loss_verif\n",
    "            patience_counter = 0\n",
    "            torch.save(PPLSTM_model.state_dict(), model_save_path_LSTM)\n",
    "            torch.save(optimizer.state_dict(), optimizer_save_path_LSTM)\n",
    "        else :\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping triggered at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        loss_conv_f = file.readlines()\n",
    "    # Parsing the data to a DataFrame\n",
    "    loss_conv = {\n",
    "        'Epoch': [],\n",
    "        'Loss_train': [],\n",
    "        'Loss_verif': []\n",
    "    }\n",
    "\n",
    "    # Split each line and extract values\n",
    "    for line in loss_conv_f:\n",
    "        parts = line.strip().split(', ')\n",
    "        epoch = int(parts[0].split(' ')[1])\n",
    "        loss_train = float(parts[1].split(': ')[1])\n",
    "        loss_verif = float(parts[2].split(': ')[1])\n",
    "        \n",
    "        loss_conv['Epoch'].append(epoch)\n",
    "        loss_conv['Loss_train'].append(loss_train)\n",
    "        loss_conv['Loss_verif'].append(loss_verif)\n",
    "\n",
    "    df = pd.DataFrame(loss_conv)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(df['Epoch'], df['Loss_train'], label='Loss on Train')\n",
    "    plt.plot(df['Epoch'], df['Loss_verif'], label='Loss on Verif')\n",
    "    plt.title(f'Loss Convergence Curve - LSTM_Seq2Seq trained with Time Window Length {seq_len} back and {seq_len_forward} forward')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.xticks(np.arange(1, 20, 1))\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.ylim(0, max(df['Loss_train']))\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'LSTM_S2S Loss Convergence r{seq_len}_f{seq_len_forward}.pdf', format='pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_TraderT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
